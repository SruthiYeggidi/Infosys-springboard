{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vuAoIUFTgIxN",
        "outputId": "a36b4663-50c4-4059-c633-7c422c88d32e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Words: ['He', 'was', 'studies', 'and', 'beautifully', 'explained', 'it', 'easily', '.']\n",
            "After stemming: ['he', 'wa', 'studi', 'and', 'beauti', 'explain', 'it', 'easili', '.']\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "# Create stemmer\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "# Sample text\n",
        "\n",
        "text = \"He was studies and beautifully explained it easily.\"\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "# Apply stemming\n",
        "stemmed_words = [stemmer.stem(word) for word in tokens]\n",
        "\n",
        "print(\"Original Words:\", tokens)\n",
        "print (\"After stemming:\", stemmed_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UsEQMUrzH07E",
        "outputId": "c70c9574-28b2-4e7a-d7dd-eea9f595e72d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens: ['hey', 'my', 'name', 'is', 'sruthi']\n",
            "Tokens 2: ['hey', 'my', 'name', 'is', 'sruthi']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "from nltk.tokenize import word_tokenize\n",
        "#sample text\n",
        "text = \"data science is fun!\"\n",
        "text2 = \"hey my name is sruthi\"\n",
        "#TT\n",
        "tokens = word_tokenize(text)\n",
        "tokens = word_tokenize(text2)\n",
        "print(\"Tokens:\", tokens)\n",
        "print(\"Tokens 2:\", tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "50o-d25sF4Q2",
        "outputId": "9f1208ba-d1a4-492e-84ea-a5fb9c9b53c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens: ['Data', 'science', 'is', 'fun', '!']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt') #download tokenizer data\n",
        "from nltk.tokenize import word_tokenize\n",
        "text=\"Data science is fun!\"\n",
        "#Tokenize Text\n",
        "tokens = word_tokenize(text)\n",
        "print(\"Tokens:\", tokens) #1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IGGb6CaQ9Xut",
        "outputId": "04eb44d6-0b02-4800-973a-6f5c38dc0faf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original words: ['Artifical', 'Intelligence', 'is', 'transforming', 'the', 'world', '.']\n",
            "After Stemming: ['artif', 'intellig', 'is', 'transform', 'the', 'world', '.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "#Create stemmer\n",
        "stemmer = PorterStemmer()\n",
        "# Sample text\n",
        "text = \"Artifical Intelligence is transforming the world.\"\n",
        "tokens = word_tokenize(text)\n",
        "# Apply stemming\n",
        "stemmed_words = [stemmer.stem(word) for word in tokens]\n",
        "print(\"Original words:\", tokens)\n",
        "print(\"After Stemming:\", stemmed_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6HtnZY2824ud",
        "outputId": "8babab9c-55ca-4664-dc0e-6419912c8c81"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens: ['Data', 'science', 'is', 'fun', '!']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt') #download tokenizer data\n",
        "from nltk.tokenize import word_tokenize\n",
        "text=\"Data science is fun!\"\n",
        "#TT\n",
        "tokens = word_tokenize(text)\n",
        "print(\"Tokens:\", tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lpwvW20uE2Te",
        "outputId": "e3e9a883-8266-431c-a015-6a1df95ceed9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Words: ['The', 'studies', 'were', 'running', 'better', 'than', 'expected', '.']\n",
            "After Lemmatization: ['The', 'study', 'were', 'running', 'better', 'than', 'expected', '.']\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "# Create lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "# Sample text\n",
        "text = \"The studies were running better than expected.\"\n",
        "tokens = word_tokenize(text)\n",
        "#Apply lemmatization\n",
        "lem_words = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "print(\"Original Words:\", tokens)\n",
        "print(\"After Lemmatization:\", lem_words)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}